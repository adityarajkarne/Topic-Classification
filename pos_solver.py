###################################
# CS B551 Fall 2016, Assignment #3
#
# Your names and user ids:
# Stephen Giuliani (sgiulian)
# Aditya Rajkarne (advrajka)

# (Based on skeleton code by D. Crandall)
#
#
####
# Put your report here!!
"""
1.2
We have to keep sampling  P(Wn+1 |Wn = wn) to generate words until we hit a period, exclamation point, or question mark.
So we extracted first words in every line, creating a list of all initial words. To sample from it we called
random.choice(), which is similar as traditional sampling as the probability of a word being picked is directly
proportional to the number of times it occurs in the first words' list. Then we accumulate all the words that have been
mentioned after that word including repeats. We call random.choice again to pick a word from the list. This goes on
until the stopping condition is met and we are left with a generated sentence. If we a ';' is chosen, we evaluate if
the prior word in the generated sentence is also a ';'. If we are expecting two ";" in a row, it evaluates the
probability of getting a ;; versus the lowest probability of any otehr word. If the lowest probability of getting any
other word is higher than ;;, then the code randomly selects from a list of all words in the training data. Otherwise,
the ';' will be added.

1.1 and 1.3
Here, we compared the probability of the given sentence under the Markov Chain to the probabilities of the sentences
generated by switching the word to another in the confusion group a brief. First, we check all the words in the sentence
to see which words can they be confused with. We count the number of instances where a sequence(a word following the
word) is also repeated in the training and set.plist stores it. We then calculate probabilities in pfinals using plist
and multiplying all probabilities in pfinals gives the probability of the sentence. We calculate that for all possible
combinations of the sentence that can be formed by replacing words that can be confused.


"""

print "Needs report at top"
####

import random
import math

# EXPAND TO SEE CODE
def do_part12(argv):
    	import random

    	file = open(argv[2])
	lines = file.readlines()
	firsts=[]
	for i in range(len(lines)):
	    lines[i]=lines[i].strip().split(" ")
	    firsts.append(lines[i][0])


	print("--------------------------------------------part 2 output-------------------------------")
	for i in range(5):
	    counter=0
	    next=random.choice(firsts)
	    sentence=[next]
	    while next not in (".","!","?"):
		nexts=[]
		for i in range(len(lines)):
		    try:
		        if next in lines[i]:
		            nexts=nexts+[lines[i][index+2] for index, word in enumerate(lines[i][:-2]) if word == next]
		    except IndexError:
		        pass
		next=random.choice(nexts)
		if next==";" :
		    counter+=1
		    if counter>5:
		        while next!=";":
		            next = random.choice(sentence)
		sentence.append(next)

	    print(" ".join(sentence))



# EXPAND TO SEE CODE
def do_part13(argv):
    import itertools
    usr_input = argv[3]
    usr_input = usr_input.lower().split(' ')
    original = usr_input[:]
    usr_set = set(usr_input)

    # finding subs
    subs = {}  # will store word as a key and a list of confused words as value
    file = open("confused_words.txt", "r")
    lines = file.readlines()

    for i in range(len(lines)):
        lines[i] = lines[i].strip().split(" ")
        for j in range(len(lines[i])):
            if lines[i][j] in usr_set:
                subs[lines[i][j]] = lines[i]
    indices = [i for i in range(len(usr_input)) if
               usr_input[i] in subs]  # gets all indices that represent confused words

    # print indices


    def pcalculator(gen_input):
        gen_set = set(gen_input)
        # get counts
        counter = {}  # stores individual counts
        plist = [0] * (len(gen_input) - 1)  # stores all transition probabilities,p12 stored in index1
        ftrain = open("bc.train", 'r')
        train = ftrain.readlines()
        firsts = []  # stores all first words of the sentence
        for i in range(len(train)):
            train[i] = train[i].strip().lower().split(' ')
            firsts.append(train[i][0])
            for j in range(len(train[i])):
                if train[i][j] in gen_set:
                    if train[i][j] not in counter:
                        counter[train[i][j]] = 1
                    else:
                        counter[train[i][j]] += 1
                    try:
                        if train[i][j + 2] == gen_input[gen_input.index(
                                train[i][j]) + 1]:  # if next word in text is the next word in sentence
                            plist[gen_input.index(train[i][j + 2]) - 1] += 1
                    except IndexError:
                        pass

        # #calculate p
        # print "gen_input:",gen_input
        # print "firsts: ",firsts
        pfinals = [float(firsts.count(gen_input[0])) / float(len(firsts))] + [plist[i] / counter[gen_input[i]] for i in
            range(len(plist))]  # po,follwed by transition probabilities
        # print "pfinals: ",pfinals
        z = sorted(set(pfinals))[1] / len(firsts)  # for not found cases min/length
        pmod = [z if pfinals[i] == 0 else pfinals[i] for i in range(len(pfinals))]
        p = 1
        for elem in pmod:
            p *= elem
        return p

    change = []
    for word in usr_input:
        if word in subs:
            change.append(subs[word])
    combo = list(itertools.product(*change))  # generates all possible combinations that can be inserted in the sentence

    suggest = 0
    best = []
    for tups in combo:
        # print(tups)
        for (k, v) in list(zip(indices, tups)):
            usr_input[k] = v
        new = pcalculator(usr_input)
        # print("new",new,best)
        if new > suggest:
            suggest = new
            # print("suggest",suggest)
            best = usr_input[:]
            print"Testing another sentence: ", " ".join(best), " ...with a probability of: ", new
    if best == original:
        print"This seems alright!"
    else:
        print'\nThe original sentence:   "', " ".join(
            usr_input), '"   should be changed. It has a probability of only', pcalculator(usr_input)
        print'Instead, we suggest:   "', " ".join(best), '"   with a probability of', pcalculator(best)



# We've set up a suggested code structure, but feel free to change it. Just
# make sure your code still works with the label.py and pos_scorer.py code
# that we've supplied.
#
class Solver:
    # General note: pos = 'Parts of speech'
    initial = {}
    transition = {}
    emission = {}
    words_dict = {}
    parts_dict = {}
    pos_prob = {}

    trans_count = 0
    sent_len = []

    ## Creates a dictionary with the probability as values
    """
    def prob(self, input_data):
        for i in range(len(input_data)):
            total = sum(input_data[i].values())
            for x in input_data[i].keys():
                input_data[i][x] = (input_data[i][x], input_data[i][x] / total)
        return input_data
    """

    # logarithmic posterior probability
    def posterior(self, sentence, pos):
        return 0
        """
        sum = float(0)
        temp_calc = math.log(1)
        for i in range(len(sentence)):
            emmission = self.emission[(sentence[i], pos[i])]
            if emmission == 0:
                emmission = 1.00000 / 10.00000 ** 10
            sum += emmission * self.pos_prob[pos[i]]
            temp_calc += math.log(emmission * self.pos_prob[pos[i]])
            posterior_prob = temp_calc - math.log(sum)
        return posterior_prob
        """

    # Do the training!
    #
    def train(self, data):
        """
        self.transition = {}
        self.emission = {}
        self.words_dict = {}
        self.parts_dict = {}
        self.trans_count = 0

        def prob(input_data):
            for i in range(len(input_data)):
                total = sum(input_data[i].values())
                for x in input_data[i].keys():
                    input_data[i][x] = (input_data[i][x], input_data[i][x] / total)
            return input_data

        for i in data:
            #learning from each word of given sentence
            for x in range(len(i[0])):
                try:    #length of longest sentence
                    self.sent_len[x][i[1][x]] = self.sent_len[x].get(i[1][x], 0) + 1.0
                except IndexError:
                    self.sent_len += [{i[1][x]:1.0}]  #length of 1, otherwise
                #counts the words
                if i[0][x] not in self.words_dict:
                    self.words_dict[i[0][x]] = 1.0
                else:
                    self.words_dict[i[0][x]] += 1.0
                #counts the POS
                if i[1][x] not in self.parts_dict:
                    self.parts_dict[i[1][x]] = 1.0
                else:
                    self.parts_dict[i[1][x]] += 1.0
                #Initial POS
                if x == 0:
                    if i[1][x] not in self.initial:
                        self.initial[i[1][x]] = 1.0
                    else:
                        self.initial[i[1][x]] += 1.0
                else:   #transitions
                    temp_trans = (i[1][x-1], i[1][x])

                    if temp_trans not in self.transition:
                        self.transition[temp_trans] = 1.0
                        self.trans_count += 1.0
                    else:
                        self.transition[temp_trans] += 1.0
                        self.trans_count += 1.0
                #counts of all the emissions
                if (i[0][x], i[1][x]) not in self.emission:
                    self.emission[(i[0][x], i[1][x])] = 1.0
                else:
                    self.emission[(i[0][x], i[1][x])] += 1.0
        ## Emission prob
        for i in self.words_dict.keys():
            for x in self.parts_dict.keys():
                if (i, x) in self.emission:
                    self.emission[(i, x)] /= self.parts_dict[x]
        ## Initial probs
        for i in self.parts_dict.keys():
            self.initial[i] /= len(data)
        ## Simple prob of POS
        for i in self.parts_dict.keys():
            self.pos_prob[i] = self.parts_dict[i] / sum(self.parts_dict.values())
        ## Transition Probs
        for x in self.parts_dict.keys():
            for y in self.parts_dict.keys():
                if (x, y) not in self.transition:
                    self.transition[(x, y)] = 1.000/10.000**10
                else:
                    self.transition[(x, y)] /= self.parts_dict[x]
        #Assumes that if the word wasn't found or tagged in the training file that it was 'itching' to show up
        #Therefore, at 1/(all words + 1) it's assumed to had been the very next word that didn't make the cut...
        for i in range(len(self.sent_len)):
            if len(self.parts_dict.keys()) != len(self.sent_len[i].keys()):
                for pos in set(self.parts_dict.keys()) - set(self.sent_len[i].keys()):
                    self.sent_len[i][pos] = 1/(len(self.words_dict.keys())+1)
        self.sent_len = prob(self.sent_len)
        """

    # Functions for each algorithm.
    #
    def simplified(self, sentence):

        def read_data(fname):
            exemplars = []
            file = open(fname, 'r');
            for line in file:
                data = tuple([w.lower() for w in line.split()])
                exemplars += [(data[0::2], data[1::2]), ]

            return exemplars

        start = read_data('bc.test')

        ## Start of the training portion
        def training(data):
            all_words = []
            all_parts = []
            sentences = []
            parts = []
            for x in data:
                sentences.append(x[0])  # creates sentences
                parts.append(x[1])  # creates parts of speech fo each sentence
            for i in sentences:
                for i in i:
                    all_words.append(i)  # list of every word (not set)
            for i in parts:
                for i in i:
                    all_parts.append(i)  # list of every part of speech (not set)

            words_set = list(set(all_words))  # word set if needed
            parts_set = list(set(all_parts))  # parts os speech set if needed

            initial_words = [i[0] for i in sentences]  # initial words, from all_words
            initial_parts = [i[0] for i in parts]  # initial pos, from all_parts

            ## calculated the maximum of a list or list of lists
            def max_calc(list):
                maximum = [0, 0]
                for i in list:
                    if i[1] > maximum[1]:
                        maximum = i
                return maximum

            def init_word_prob(word):
                return [word,
                        float(initial_words.count(word)) / float(len(initial_words))]  # initial probability of a word

            def init_part_prob(part):
                return [part,
                        float(initial_parts.count(part)) / float(len(initial_parts))]  # initial probability of a pos

            def word_prob(word):
                return [word,
                        float(all_words.count(word)) / float(len(all_words))]  # probability the word exists --> P(w)

            def parts_prob(part):
                return [part,
                        float(all_parts.count(part)) / float(len(all_parts))]  # probability the pos exists --> P(s)

            def word_part_list(sample):  # creates a list of all words and their respective pos -->  [word,pos]
                matched = []
                for i in sample:
                    for n in range(len(i[0])):
                        matched.append([i[0][n], i[1][n]])
                return matched

            ## this one below may need to be tweaked. i think this is usefull for the assignment (if the sentence is provided, this is part of calculating the most probable POS
            def parts_give_word(word):  # returns a list of all the parts of speech if the word is supplied
                parts_found = []
                for i in word_part_list(data):
                    if i[0] == word:
                        parts_found.append(i[1])
                return parts_found

            # pgw = "Part of speech given word"


            def pgw_calc(
                    word):  # this calculates the probabilities of the pos given the word (used formula above) --> returns only the maximum, but doesn't use the max def at the top
                probs = []
                for i in parts_give_word(word):
                    probs.append([i, float(parts_give_word(word).count(i)) / float(len(parts_give_word(word)))])
                maximum = [0, 0]
                for i in probs:
                    if i[1] > maximum[0]:
                        maximum = i
                return maximum

            def words_give_parts(
                    part):  # this is the same as the calculations above but for words given the part of speech
                words_found = []
                for i in word_part_list(data):
                    if i[1] == part:
                        words_found += i[0]
                return words_found

            # wgp = "word given pos"
            def wgp_calc(part):
                probs = []
                for i in words_give_parts(part):
                    probs.append([i, float(words_give_parts(part).count(i)) / float(len(words_give_parts(part)))])
                maximum = [0, 0]
                for i in probs:
                    if i[1] > maximum[0]:
                        maximum = i
                return maximum

            def trial(test):  # This was the initial calculations for the input sentence, it uses pgw_calc
                output = []
                for i in test:
                    output.append((pgw_calc(i)[0]))
                return output

            print"Thinking..."
            output= " ".join(trial(sentence.lower().split(' ')))

            ## Calculates the most likely initial part of speech
            #t = [init_part_prob(i) for i in parts_set]
            # print max_calc(t)

            return output

    def hmm_ve(self, sentence):
        return [ "noun" ] * len(sentence)

    def hmm_viterbi(self, sentence):
        return ["noun"] * len(sentence)
        """
        print_out = []
        temp_dict = {}
        pos_sentence = {} #pos_sentence is a sentence made of only POSs
        #wors on initial word
        for POS in self.parts_dict.keys():
            if (sentence[0], POS) not in self.emission:
                self.emission[(sentence[0], POS)] = 1/(len(self.emission)+1) #as if it would have been the next emission
            prob = float(self.initial[POS] * self.emission[(sentence[0], POS)])
            temp_dict[(POS, 0)] = prob
        #works the rest of the sentence for potential POSs and compares them --NOT LAST WORD IN SENTENCE
        for x in range(1, len(sentence)):
            for potential_pos1 in self.parts_dict.keys():
                max_prob = float(0)
                likely_pos = None
                for potential_pos2 in self.parts_dict.keys():
                    prob = temp_dict[(potential_pos2, x-1)] * self.transition[(potential_pos2, potential_pos1)]
                    if prob > max_prob:
                        max_prob = prob
                        likely_pos = potential_pos2
                #if probability is zero then tag 'noun' POS
                if max_prob == 0:
                    max_prob = 0.001
                    likely_pos = 'noun'  #independent analysis showed noun at 18.5% recurrance, so we assigned
                                         #the default value on 'noun' and bumped the max prob to .001 just in case
                temp_dict[(potential_pos1, x)] = self.emission[(sentence[x], potential_pos1)] * max_prob
                pos_sentence[(potential_pos1, x)] = likely_pos
        sent_len = len(sentence) - 1
        max = float(0)
        for POS in self.parts_dict.keys():  #here's where we calculate the last word
            if temp_dict[(POS, sent_len)] > max:
                max = temp_dict[(POS, sent_len)]
                likely_pos = POS
        print_out.append(likely_pos)
        sent_len = len(sentence)

        for i in range(sent_len-1):
            POS = pos_sentence[(likely_pos, sent_len-i-1)]
            likely_pos = POS
            print_out.append(POS)

        print_out.reverse()
        return [ [print_out], [] ]

    """
    # This solve() method is called by label.py, so you should keep the interface the
    #  same, but you can change the code itself.
    # It's supposed to return a list with two elements:
    #
    #  - The first element is a list of part-of-speech labelings of the sentence.
    #    Each of these is a list, one part of speech per word of the sentence.
    #
    #  - The second element is a list of probabilities, one per word. This is
    #    only needed for simplified() and complex() and is the marginal probability for each word.
    #
    def solve(self, algo, sentence):
        if algo == "Simplified":
            return self.simplified(sentence)
        elif algo == "HMM VE":
            return self.hmm_ve(sentence)
        elif algo == "HMM MAP":
            return self.hmm_viterbi(sentence)
        else:
            print "Unknown algo!"

